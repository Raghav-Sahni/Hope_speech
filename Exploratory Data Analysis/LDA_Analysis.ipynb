{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference - https://www.youtube.com/watch?v=TKjjlp5_r7o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "folder_path = r'Data\\PreprocessedData'\n",
    "net_path = os.path.join(parent_dir, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_english = pd.read_csv(os.path.join(net_path, 'english_train_preprocess.csv'))\n",
    "df_dev_english = pd.read_csv(os.path.join(net_path, 'english_dev_preprocess.csv'))\n",
    "df_test_english = pd.read_csv(os.path.join(net_path, 'english_test_preprocess.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_english_sentences = df_train_english['preprocessed_text'].tolist()\n",
    "dev_english_sentences = df_dev_english['preprocessed_text'].tolist()\n",
    "test_english_sentences = df_test_english['preprocessed_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = train_english_sentences + dev_english_sentences + test_english_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return (texts_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_texts = lemmatization(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tiktok', 'radiate', 'gay', 'chaotic', 'energy', 'love']\n"
     ]
    }
   ],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "print(data_words[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "chaotic\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "corpus = []\n",
    "for text in data_words:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "\n",
    "print (corpus[0][0:20])\n",
    "\n",
    "word = id2word[[0][:1][0]]\n",
    "print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.071*\"matter\" + 0.067*\"life\" + 0.055*\"people\" + 0.052*\"black\" + 0.046*\"say\" + 0.023*\"white\" + 0.016*\"other\" + 0.015*\"only\" + 0.014*\"live\" + 0.013*\"racist\"'), (1, '0.022*\"love\" + 0.021*\"get\" + 0.017*\"see\" + 0.016*\"know\" + 0.012*\"girl\" + 0.012*\"even\" + 0.012*\"agree\" + 0.012*\"time\" + 0.011*\"give\" + 0.011*\"good\"'), (2, '0.026*\"just\" + 0.026*\"so\" + 0.020*\"go\" + 0.018*\"think\" + 0.015*\"make\" + 0.014*\"want\" + 0.011*\"need\" + 0.010*\"man\" + 0.010*\"right\" + 0.010*\"thing\"')]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el279442143890033904578894401\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el279442143890033904578894401_data = {\"mdsDat\": {\"x\": [0.129095737038459, 0.1853733395046296, -0.3144690765430886], \"y\": [0.29569723142296844, -0.2631908749609015, -0.032506356462066996], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [39.1230684946277, 31.92142364129213, 28.955507864080165]}, \"tinfo\": {\"Term\": [\"matter\", \"life\", \"people\", \"so\", \"go\", \"think\", \"just\", \"matter\", \"life\", \"black\", \"people\", \"love\", \"see\", \"know\", \"get\"], \"Freq\": [4865.0, 4591.0, 3756.0, 2136.8921788914686, 1688.4425673510673, 1503.8763741014495, 2204.2711053152425, 4864.914495406866, 4590.405360225266, 3553.691854555573, 3755.219792832674, 1387.8012460417563, 1055.2125289385856, 979.2557492528382, 1330.2249910901758], \"Total\": [4865.0, 4591.0, 3756.0, 2137.641155610933, 1689.2161612857444, 1504.652040050753, 2211.1621940284253, 4865.612790892473, 4591.106972342833, 3554.3987707820042, 3756.008774422252, 1388.5630979139557, 1056.024553255566, 980.0172424829664, 1698.3367637770943], \"Category\": [\"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [3.0, 2.0, 1.0, -3.6675, -3.9031, -4.0188, -3.6365, -2.6414, -2.6995, -2.9555, -2.9003, -3.7982, -4.0722, -4.1469, -3.8406], \"loglift\": [3.0, 2.0, 1.0, 0.9381, 0.938, 0.9379, 0.9353, 1.1417, 1.1417, 1.1417, 1.1417, 1.2389, 1.2386, 1.2386, 0.9951]}, \"token.table\": {\"Topic\": [2, 2, 3, 1, 1, 2, 3, 2, 3, 2, 2, 3, 1, 1], \"Freq\": [0.9998878092167705, 0.21668258489651335, 0.7831191247618553, 0.9992800440147229, 0.9967608916036246, 0.003165756007815505, 0.9989620157290405, 0.9997588877040982, 0.9995944743780086, 0.9998740567902116, 0.999731423837686, 0.999029801672312, 0.9997000639657175, 0.9995666506053247], \"Term\": [\"black\", \"get\", \"get\", \"go\", \"just\", \"just\", \"know\", \"life\", \"love\", \"matter\", \"people\", \"see\", \"so\", \"think\"]}, \"R\": 3, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el279442143890033904578894401\", ldavis_el279442143890033904578894401_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el279442143890033904578894401\", ldavis_el279442143890033904578894401_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el279442143890033904578894401\", ldavis_el279442143890033904578894401_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.129096  0.295697       1        1  39.123068\n",
       "0      0.185373 -0.263191       2        1  31.921424\n",
       "1     -0.314469 -0.032506       3        1  28.955508, topic_info=       Term         Freq        Total Category  logprob  loglift\n",
       "13   matter  4865.000000  4865.000000  Default   3.0000   3.0000\n",
       "12     life  4591.000000  4591.000000  Default   2.0000   2.0000\n",
       "26   people  3756.000000  3756.000000  Default   1.0000   1.0000\n",
       "32       so  2136.892179  2137.641156   Topic1  -3.6675   0.9381\n",
       "150      go  1688.442567  1689.216161   Topic1  -3.9031   0.9380\n",
       "136   think  1503.876374  1504.652040   Topic1  -4.0188   0.9379\n",
       "41     just  2204.271105  2211.162194   Topic1  -3.6365   0.9353\n",
       "13   matter  4864.914495  4865.612791   Topic2  -2.6414   1.1417\n",
       "12     life  4590.405360  4591.106972   Topic2  -2.6995   1.1417\n",
       "16    black  3553.691855  3554.398771   Topic2  -2.9555   1.1417\n",
       "26   people  3755.219793  3756.008774   Topic2  -2.9003   1.1417\n",
       "3      love  1387.801246  1388.563098   Topic3  -3.7982   1.2389\n",
       "286     see  1055.212529  1056.024553   Topic3  -4.0722   1.2386\n",
       "46     know   979.255749   980.017242   Topic3  -4.1469   1.2386\n",
       "8       get  1330.224991  1698.336764   Topic3  -3.8406   0.9951, token_table=      Topic      Freq    Term\n",
       "term                         \n",
       "16        2  0.999888   black\n",
       "8         2  0.216683     get\n",
       "8         3  0.783119     get\n",
       "150       1  0.999280      go\n",
       "41        1  0.996761    just\n",
       "41        2  0.003166    just\n",
       "46        3  0.998962    know\n",
       "12        2  0.999759    life\n",
       "3         3  0.999594    love\n",
       "13        2  0.999874  matter\n",
       "26        2  0.999731  people\n",
       "286       3  0.999030     see\n",
       "32        1  0.999700      so\n",
       "136       1  0.999567   think, R=3, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=3)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tiktok', 'radiate', 'gay', 'chaotic', 'energy', 'love']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tiktok', 'radiate', 'gay', 'chaotic', 'energy', 'love']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.147*\"girl\" + 0.097*\"guy\" + 0.084*\"do\" + 0.074*\"day\" + 0.056*\"happen\" + 0.053*\"boy\" + 0.052*\"big\" + 0.044*\"christian\" + 0.036*\"friend\" + 0.029*\"fuck\"'), (1, '0.347*\"just\" + 0.153*\"live\" + 0.089*\"never\" + 0.072*\"human\" + 0.069*\"feel\" + 0.069*\"like\" + 0.020*\"proud\" + 0.018*\"shut\" + 0.017*\"emoji\" + 0.013*\"base\"'), (2, '0.360*\"get\" + 0.101*\"kill\" + 0.094*\"use\" + 0.067*\"back\" + 0.060*\"put\" + 0.047*\"straight\" + 0.041*\"again\" + 0.037*\"money\" + 0.031*\"liberal\" + 0.008*\"false\"'), (3, '0.185*\"more\" + 0.135*\"thing\" + 0.127*\"agree\" + 0.104*\"person\" + 0.063*\"don\" + 0.050*\"lol\" + 0.042*\"homosexuality\" + 0.035*\"amazing\" + 0.024*\"school\" + 0.019*\"funny\"'), (4, '0.252*\"see\" + 0.172*\"time\" + 0.163*\"good\" + 0.080*\"ve\" + 0.071*\"color\" + 0.030*\"idea\" + 0.020*\"clearly\" + 0.018*\"side\" + 0.014*\"dog\" + 0.005*\"upset\"'), (5, '0.133*\"tell\" + 0.103*\"show\" + 0.095*\"there\" + 0.093*\"lot\" + 0.053*\"whole\" + 0.047*\"job\" + 0.046*\"political\" + 0.044*\"move\" + 0.038*\"teach\" + 0.035*\"medium\"'), (6, '0.255*\"come\" + 0.242*\"race\" + 0.157*\"still\" + 0.051*\"war\" + 0.021*\"set\" + 0.019*\"though\" + 0.007*\"stage\" + 0.000*\"other\" + 0.000*\"point\" + 0.000*\"privilege\"'), (7, '0.275*\"wrong\" + 0.074*\"beat\" + 0.064*\"bible\" + 0.062*\"save\" + 0.046*\"music\" + 0.035*\"laugh\" + 0.000*\"book\" + 0.000*\"other\" + 0.000*\"parade\" + 0.000*\"message\"'), (8, '0.394*\"say\" + 0.123*\"know\" + 0.102*\"right\" + 0.093*\"then\" + 0.043*\"all\" + 0.033*\"end\" + 0.031*\"truth\" + 0.023*\"therapy\" + 0.015*\"gay\" + 0.013*\"statement\"'), (9, '0.214*\"think\" + 0.109*\"year\" + 0.088*\"try\" + 0.058*\"fact\" + 0.049*\"believe\" + 0.037*\"opinion\" + 0.035*\"lesbian\" + 0.027*\"pure\" + 0.026*\"evil\" + 0.025*\"less\"'), (10, '0.256*\"matter\" + 0.242*\"life\" + 0.187*\"black\" + 0.083*\"white\" + 0.026*\"talk\" + 0.021*\"true\" + 0.020*\"as\" + 0.019*\"movement\" + 0.015*\"cop\" + 0.013*\"seem\"'), (11, '0.336*\"go\" + 0.086*\"protest\" + 0.061*\"let\" + 0.061*\"care\" + 0.044*\"name\" + 0.031*\"victim\" + 0.030*\"eye\" + 0.023*\"literally\" + 0.020*\"turn\" + 0.019*\"native\"'), (12, '0.101*\"need\" + 0.095*\"man\" + 0.088*\"really\" + 0.087*\"look\" + 0.060*\"woman\" + 0.053*\"very\" + 0.053*\"stop\" + 0.047*\"most\" + 0.038*\"engineer\" + 0.032*\"understand\"'), (13, '0.414*\"so\" + 0.174*\"racist\" + 0.114*\"also\" + 0.078*\"actually\" + 0.043*\"lgbtq\" + 0.022*\"consider\" + 0.016*\"hell\" + 0.000*\"other\" + 0.000*\"many\" + 0.000*\"statue\"'), (14, '0.224*\"love\" + 0.134*\"mean\" + 0.086*\"much\" + 0.086*\"kid\" + 0.074*\"thank\" + 0.065*\"madonna\" + 0.039*\"hear\" + 0.037*\"respect\" + 0.037*\"help\" + 0.027*\"joke\"'), (15, '0.841*\"people\" + 0.000*\"other\" + 0.000*\"too\" + 0.000*\"hate\" + 0.000*\"take\" + 0.000*\"racism\" + 0.000*\"change\" + 0.000*\"give\" + 0.000*\"many\" + 0.000*\"statue\"'), (16, '0.135*\"only\" + 0.103*\"now\" + 0.102*\"even\" + 0.071*\"blm\" + 0.060*\"video\" + 0.055*\"here\" + 0.053*\"same\" + 0.051*\"comment\" + 0.046*\"support\" + 0.040*\"watch\"'), (17, '0.189*\"make\" + 0.180*\"want\" + 0.093*\"well\" + 0.088*\"re\" + 0.067*\"country\" + 0.064*\"one\" + 0.063*\"own\" + 0.038*\"already\" + 0.027*\"family\" + 0.018*\"doesn\"'), (18, '0.000*\"thisnwhen\" + 0.000*\"nleave\" + 0.000*\"crticism\" + 0.000*\"pacific\" + 0.000*\"sl_ave\" + 0.000*\"propel\" + 0.000*\"shrodinger\" + 0.000*\"torus\" + 0.000*\"smmh\" + 0.000*\"publically\"'), (19, '0.128*\"way\" + 0.121*\"police\" + 0.112*\"world\" + 0.092*\"user\" + 0.083*\"work\" + 0.048*\"keep\" + 0.045*\"hair\" + 0.043*\"short\" + 0.033*\"sorry\" + 0.022*\"abuse\"')]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
